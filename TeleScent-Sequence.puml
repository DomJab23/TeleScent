@startuml TeleScent Sequence Diagram

skinparam backgroundColor #FFFFFF
skinparam sequenceMessageAlign center

title TeleScent System - Sequence Diagram\nSensor Reading to Scent Emission Flow

actor User
participant "Web Browser" as Browser
participant "eNose Device" as eNose
participant "GSM Network" as GSM
participant "Backend Server\n(Express)" as Backend
participant "Data Store\n(Memory)" as DataStore
participant "ML Service\n(Python)" as ML
participant "Scent Emitter" as Emitter

== User Authentication ==
User -> Browser: Opens TeleScent App
Browser -> Backend: POST /api/auth/login\n{username, password}
activate Backend
Backend -> DataStore: Query user credentials
DataStore --> Backend: User data
Backend --> Browser: JWT Token
deactivate Backend
Browser -> Browser: Store token in localStorage

== Sensor Data Collection & Transmission ==
eNose -> eNose: Collect sensor readings\n(BME680, SGP41, GMXXX)
eNose -> eNose: Format as JSON payload
eNose -> GSM: HTTP POST request
activate GSM
GSM -> Backend: POST /api/sensor-data\n{device_id, sensors...}
deactivate GSM

== Backend Processing ==
activate Backend
Backend -> Backend: Validate request
Backend -> DataStore: Store sensor data
activate DataStore
DataStore --> Backend: Stored successfully
deactivate DataStore

== Real-time Update to Frontend ==
Backend -> Browser: SSE: New sensor data
activate Browser
Browser -> Browser: Update dashboard charts
Browser -> Browser: Display current values
deactivate Browser

== ML Prediction Process ==
Backend -> Backend: Check VOC/NO2 changes

alt Significant drop detected
    Backend -> Backend: Force prediction = "no_scent"
else Normal reading
    Backend -> ML: Spawn Python process\nStdin: {sensor_data}
    activate ML
    
    ML -> ML: Detect available sensors
    
    alt 6 sensors available
        ML -> ML: Load full model\n(scent_pipeline.joblib)
    else 2 sensors only
        ML -> ML: Load simple model\n(simple_2sensor_model.joblib)
    end
    
    ML -> ML: Preprocess data
    ML -> ML: Run prediction
    ML -> ML: Apply consecutive logic\n(3x confirmation)
    ML -> ML: Calculate confidence
    
    ML --> Backend: Stdout: {predicted_scent,\nconfidence, probabilities}
    deactivate ML
end

Backend -> DataStore: Store prediction
activate DataStore
DataStore --> Backend: Stored
deactivate DataStore

== Scent Emission Control ==
Backend -> Backend: Map scent to channel\n(cinnamon=0, vanilla=3, etc)
Backend -> Backend: Calculate PWM intensity\n(base 200, scaled by confidence)
Backend -> DataStore: Store emitter control\n{0:174, 1:0, 2:0...}
activate DataStore
DataStore --> Backend: Stored
deactivate DataStore

note over Emitter: Emitter polls backend\nfor control commands

Emitter -> Backend: GET /api/sensor-data/emitter\nor GET /api/predictions
activate Backend
Backend -> DataStore: Retrieve emitter control
DataStore --> Backend: Control data
Backend --> Emitter: HTTP 200 OK\n{0:174, 1:0, 2:0...}
deactivate Backend

activate Emitter
Emitter -> Emitter: Parse control data
Emitter -> Emitter: Set PWM duty cycles
Emitter -> Emitter: Activate scent cartridge
deactivate Emitter

== Update Frontend with Prediction ==
Backend -> Browser: SSE: New prediction
activate Browser
Browser -> Browser: Update ML Console
Browser -> Browser: Display predicted scent
Browser -> Browser: Show confidence level
Browser -> Browser: Update history chart
deactivate Browser

Backend --> eNose: HTTP 200 OK
deactivate Backend

== User Views Results ==
User -> Browser: Views dashboard
Browser --> User: Shows real-time data\nand predictions

@enduml
