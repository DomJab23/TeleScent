{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f85929",
   "metadata": {},
   "source": [
    "#  TeleScent Model Comparison - Finding the Best Algorithm\n",
    "\n",
    "## Goal: Find the best ML algorithm for scent detection\n",
    "\n",
    "Testing multiple algorithms:\n",
    "- Random Forest (current baseline)\n",
    "- Gradient Boosting (XGBoost, LightGBM)\n",
    "- Support Vector Machine (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Multi-Layer Perceptron (Neural Network)\n",
    "- Naive Bayes\n",
    "\n",
    "Focus: **Real-world generalization**, not just test accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0c553",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb01b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.13.2' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Multiple algorithms to test\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7b7a6",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_FILE = 'NATURAL_ML_Data_with_no_scent.xlsx'\n",
    "MODEL_DIR = Path('model')\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5\n",
    "\n",
    "print(f\"üìÅ Data file: {DATA_FILE}\")\n",
    "print(f\"üíæ Model directory: {MODEL_DIR}\")\n",
    "print(f\"üî¢ Cross-validation: {CV_FOLDS} folds\")\n",
    "print(f\"üìä Test size: {TEST_SIZE*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(f\"üìñ Loading dataset from {DATA_FILE}...\")\n",
    "df = pd.read_excel(DATA_FILE)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüéØ Classes in dataset:\")\n",
    "print(df['scent_name'].value_counts())\n",
    "\n",
    "# Show sample\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (same as current model)\n",
    "print(\"üß™ Feature Engineering...\")\n",
    "\n",
    "# Raw features\n",
    "feature_cols = ['srawVoc', 'srawNox', 'NO2', 'ethanol', 'VOC_multichannel', 'COandH2']\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "# 1. VOC RATIOS\n",
    "X['voc_ratio'] = X['VOC_multichannel'] / (X['srawVoc'] + 1)\n",
    "X['ethanol_voc_ratio'] = X['ethanol'] / (X['VOC_multichannel'] + 1)\n",
    "X['voc_balance'] = (X['VOC_multichannel'] - X['ethanol']) / (X['VOC_multichannel'] + X['ethanol'] + 1)\n",
    "\n",
    "# 2. NOx INDICATORS\n",
    "X['nox_intensity'] = X['NO2'] / (X['srawNox'] + 1)\n",
    "X['nox_balance'] = (X['NO2'] - X['srawNox'] / 100) / (X['NO2'] + X['srawNox'] / 100 + 1)\n",
    "\n",
    "# 3. GAS INTERACTIONS\n",
    "X['voc_no2_interaction'] = X['VOC_multichannel'] * X['NO2'] / 1000\n",
    "X['ethanol_no2_ratio'] = X['ethanol'] / (X['NO2'] + 1)\n",
    "X['co_voc_ratio'] = X['COandH2'] / (X['VOC_multichannel'] + 1)\n",
    "\n",
    "# 4. CHEMICAL COMPLEXITY\n",
    "X['total_voc_intensity'] = X['VOC_multichannel'] + X['ethanol'] + X['srawVoc'] / 100\n",
    "X['chemical_diversity'] = np.std(X[['NO2', 'ethanol', 'VOC_multichannel', 'COandH2']], axis=1)\n",
    "X['gas_dominance'] = X[['NO2', 'ethanol', 'VOC_multichannel', 'COandH2']].max(axis=1) / (X[['NO2', 'ethanol', 'VOC_multichannel', 'COandH2']].mean(axis=1) + 1)\n",
    "\n",
    "# 5. NORMALIZED SENSORS\n",
    "for col in ['srawVoc', 'srawNox']:\n",
    "    col_mean = X[col].mean()\n",
    "    col_std = X[col].std()\n",
    "    X[f'{col}_normalized'] = (X[col] - col_mean) / (col_std + 1)\n",
    "\n",
    "# Replace inf with NaN\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Engineered features: {X.shape[1]} total features\")\n",
    "print(f\"   Features: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55389813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = df['scent_name'].copy()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"üè∑Ô∏è  Label Encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = (y == label).sum()\n",
    "    print(f\"  {label} ‚Üí {i} ({count} samples)\")\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data split:\")\n",
    "print(f\"   Training: {X_train.shape[0]} samples\")\n",
    "print(f\"   Test: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5b2bd",
   "metadata": {},
   "source": [
    "## 4. Algorithm Comparison\n",
    "\n",
    "We'll test each algorithm with:\n",
    "1. **5-fold Cross-Validation** - More reliable than single test split\n",
    "2. **Training Time** - Important for retraining\n",
    "3. **Test Accuracy** - Final performance\n",
    "4. **Per-Class Performance** - Check if any class is failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f707a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define algorithms to test\n",
    "algorithms = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'SVM (RBF)': SVC(\n",
    "        kernel='rbf',\n",
    "        C=10,\n",
    "        gamma='scale',\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'SVM (Linear)': SVC(\n",
    "        kernel='linear',\n",
    "        C=1.0,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "        n_neighbors=7,\n",
    "        weights='distance',\n",
    "        metric='euclidean'\n",
    "    ),\n",
    "    'Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        max_iter=500,\n",
    "        random_state=RANDOM_STATE,\n",
    "        early_stopping=True\n",
    "    ),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        max_depth=20,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"üî¨ Testing {len(algorithms)} algorithms:\")\n",
    "for name in algorithms.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all algorithms\n",
    "results = []\n",
    "\n",
    "print(\"üöÄ Starting algorithm comparison...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, classifier in algorithms.items():\n",
    "    print(f\"\\nüìä Testing: {name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation\n",
    "    start_time = time.time()\n",
    "    cv_scores = cross_val_score(\n",
    "        pipeline, X_train, y_train, \n",
    "        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    cv_time = time.time() - start_time\n",
    "    \n",
    "    # Train on full training set\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Test set performance\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'Algorithm': name,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Train Time (s)': train_time,\n",
    "        'CV Time (s)': cv_time,\n",
    "        'Pipeline': pipeline\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"  CV Score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Training Time: {train_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ All algorithms tested!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82785527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame([\n",
    "    {k: v for k, v in r.items() if k != 'Pipeline'}\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# Sort by CV Mean (most reliable metric)\n",
    "results_df = results_df.sort_values('CV Mean', ascending=False)\n",
    "\n",
    "print(\"üìä ALGORITHM COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df.style.background_gradient(subset=['CV Mean', 'Test Accuracy'], cmap='RdYlGn'))\n",
    "\n",
    "# Highlight best\n",
    "best_algo = results_df.iloc[0]\n",
    "print(f\"\\nüèÜ BEST ALGORITHM: {best_algo['Algorithm']}\")\n",
    "print(f\"   CV Score: {best_algo['CV Mean']:.4f} (¬±{best_algo['CV Std']:.4f})\")\n",
    "print(f\"   Test Accuracy: {best_algo['Test Accuracy']:.4f}\")\n",
    "print(f\"   Training Time: {best_algo['Train Time (s)']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. CV Score comparison\n",
    "ax1 = axes[0, 0]\n",
    "results_df.plot(x='Algorithm', y='CV Mean', kind='barh', ax=ax1, color='steelblue', xerr='CV Std')\n",
    "ax1.set_xlabel('Cross-Validation Accuracy')\n",
    "ax1.set_title('Cross-Validation Performance (with std dev)', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Test accuracy comparison\n",
    "ax2 = axes[0, 1]\n",
    "results_df.plot(x='Algorithm', y='Test Accuracy', kind='barh', ax=ax2, color='coral')\n",
    "ax2.set_xlabel('Test Accuracy')\n",
    "ax2.set_title('Test Set Performance', fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Training time\n",
    "ax3 = axes[1, 0]\n",
    "results_df.plot(x='Algorithm', y='Train Time (s)', kind='barh', ax=ax3, color='mediumseagreen')\n",
    "ax3.set_xlabel('Training Time (seconds)')\n",
    "ax3.set_title('Training Speed', fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. CV vs Test (overfitting check)\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(results_df['CV Mean'], results_df['Test Accuracy'], s=100, alpha=0.6)\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax4.annotate(row['Algorithm'], (row['CV Mean'], row['Test Accuracy']), \n",
    "                fontsize=8, alpha=0.7)\n",
    "ax4.plot([0.9, 1.0], [0.9, 1.0], 'r--', alpha=0.5, label='Perfect agreement')\n",
    "ax4.set_xlabel('CV Mean Accuracy')\n",
    "ax4.set_ylabel('Test Accuracy')\n",
    "ax4.set_title('Overfitting Check (CV vs Test)', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39262f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model and show detailed performance\n",
    "best_result = results[results_df.index[0]]\n",
    "best_pipeline = best_result['Pipeline']\n",
    "best_name = best_result['Algorithm']\n",
    "\n",
    "print(f\"üîç DETAILED ANALYSIS: {best_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(\n",
    "    y_test, y_pred,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_name}', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüéØ Per-Class Accuracy:\")\n",
    "print(\"-\" * 80)\n",
    "for i, scent in enumerate(label_encoder.classes_):\n",
    "    mask = y_test == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "        print(f\"  {scent:15s}: {acc:.3f} ({mask.sum()} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96118a70",
   "metadata": {},
   "source": [
    "##5. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "BEST_PIPELINE_PATH = MODEL_DIR / 'scent_pipeline_best.joblib'\n",
    "BEST_LABEL_ENCODER_PATH = MODEL_DIR / 'label_encoder_best.joblib'\n",
    "\n",
    "print(f\"üíæ Saving best model: {best_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save pipeline and encoder\n",
    "joblib.dump(best_pipeline, BEST_PIPELINE_PATH)\n",
    "joblib.dump(label_encoder, BEST_LABEL_ENCODER_PATH)\n",
    "\n",
    "print(f\"‚úÖ Pipeline saved to: {BEST_PIPELINE_PATH}\")\n",
    "print(f\"‚úÖ Label encoder saved to: {BEST_LABEL_ENCODER_PATH}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'best_algorithm': best_name,\n",
    "    'cv_mean_accuracy': float(best_result['CV Mean']),\n",
    "    'cv_std_accuracy': float(best_result['CV Std']),\n",
    "    'test_accuracy': float(best_result['Test Accuracy']),\n",
    "    'training_time_seconds': float(best_result['Train Time (s)']),\n",
    "    'classes': label_encoder.classes_.tolist(),\n",
    "    'n_features': X_train.shape[1],\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'dataset': DATA_FILE,\n",
    "    'comparison_results': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_path = MODEL_DIR / 'best_model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Metadata saved to: {metadata_path}\")\n",
    "print(f\"\\nüéâ Best model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec58c3",
   "metadata": {},
   "source": [
    "## 5. Test Best Model with Real Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with real samples from each class\n",
    "print(\"üß™ Testing Best Model with Real Samples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_scents = label_encoder.classes_\n",
    "\n",
    "for scent in test_scents:\n",
    "    # Get a sample from the test set\n",
    "    scent_idx = label_encoder.transform([scent])[0]\n",
    "    test_indices = np.where(y_test == scent_idx)[0]\n",
    "    \n",
    "    if len(test_indices) > 0:\n",
    "        # Get middle sample\n",
    "        sample_idx = test_indices[len(test_indices) // 2]\n",
    "        sample_features = X_test.iloc[[sample_idx]]\n",
    "        \n",
    "        # Predict\n",
    "        pred_idx = best_pipeline.predict(sample_features)[0]\n",
    "        pred_proba = best_pipeline.predict_proba(sample_features)[0]\n",
    "        pred_scent = label_encoder.inverse_transform([pred_idx])[0]\n",
    "        \n",
    "        # Show result\n",
    "        correct = pred_scent == scent\n",
    "        emoji = \"‚úÖ\" if correct else \"‚ùå\"\n",
    "        \n",
    "        print(f\"\\n{emoji} Expected: {scent:15s} ‚Üí Predicted: {pred_scent:15s} ({pred_proba[pred_idx]:.1%})\")\n",
    "        \n",
    "        # Show top 3 predictions\n",
    "        top3_idx = np.argsort(pred_proba)[::-1][:3]\n",
    "        print(f\"   Top 3: \", end=\"\")\n",
    "        for idx in top3_idx:\n",
    "            print(f\"{label_encoder.classes_[idx]}({pred_proba[idx]:.1%}) \", end=\"\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75314cff",
   "metadata": {},
   "source": [
    "## 6. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b95974",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüèÜ Best Algorithm: {best_name}\")\n",
    "print(f\"   ‚Ä¢ Cross-Validation: {best_result['CV Mean']:.4f} (¬±{best_result['CV Std']:.4f})\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {best_result['Test Accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Training Time: {best_result['Train Time (s)']:.2f}s\")\n",
    "\n",
    "print(f\"\\nüìà Top 3 Algorithms:\")\n",
    "for i, row in results_df.head(3).iterrows():\n",
    "    print(f\"   {i+1}. {row['Algorithm']:20s} - CV: {row['CV Mean']:.4f}, Test: {row['Test Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "if best_result['CV Mean'] > 0.95:\n",
    "    print(\"   ‚úÖ Model shows excellent performance!\")\n",
    "    print(\"   ‚Üí If real-world performance is poor, issue is likely:\")\n",
    "    print(\"      ‚Ä¢ Sensor calibration differences\")\n",
    "    print(\"      ‚Ä¢ Environmental conditions (temp, humidity)\")\n",
    "    print(\"      ‚Ä¢ Data collection artifacts\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Model performance could be improved\")\n",
    "    print(\"   ‚Üí Consider:\")\n",
    "    print(\"      ‚Ä¢ Collecting more training data\")\n",
    "    print(\"      ‚Ä¢ Trying ensemble methods\")\n",
    "    print(\"      ‚Ä¢ Hyperparameter tuning\")\n",
    "\n",
    "print(f\"\\nüîÑ Next Steps:\")\n",
    "print(f\"   1. Copy best model to production:\")\n",
    "print(f\"      cp {BEST_PIPELINE_PATH} {MODEL_DIR}/scent_pipeline.joblib\")\n",
    "print(f\"      cp {BEST_LABEL_ENCODER_PATH} {MODEL_DIR}/label_encoder.joblib\")\n",
    "print(f\"   2. Restart backend server\")\n",
    "print(f\"   3. Test with real sensor data\")\n",
    "print(f\"   4. Monitor per-class accuracy\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Model comparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
